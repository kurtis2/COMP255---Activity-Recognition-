{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comp255 Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualisation - task 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and visualise the raw data \n",
    "\n",
    "def data_visulization():\n",
    "    # read dataset file\n",
    "    df = pd.read_csv('dataset_1.txt', sep=',', header=None)\n",
    "    for i in range (1,14):\n",
    "        df_activity = df[df[24] == i].values\n",
    "        # Plot the accelerometer data\n",
    "        print(\"Accelerometer for activity\" + str(i))\n",
    "        plt.plot(df_activity[500:1500, 0:3])\n",
    "        plt.show()\n",
    "        #plot the gyroscope data\n",
    "        print(\"Gyroscope for activity\" + str(i))\n",
    "        plt.plot(df_activity[500:1500, 3:6])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function call\n",
    "data_visulization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply sensoring and filtering a visulise the same data \n",
    "\n",
    "def noise_removing():\n",
    "    # Butterworth low-pass filter. \n",
    "    b, a = signal.butter(4, 0.04, 'low', analog=False)\n",
    "    df = pd.read_csv('dataset_1.txt', sep=',', header=None)\n",
    "    for i in range(1,14):\n",
    "        df_activity = df[df[24] == i].values\n",
    "    \n",
    "        for j in range(3):\n",
    "            df_activity[:,j] = signal.lfilter(b, a, df_activity[:, j])\n",
    "        # Plot the accelerometer data\n",
    "        print(\"Accelerometer for activity\" + str (i))\n",
    "        plt.plot(df_activity[500:1500, 0:3])\n",
    "        plt.show()\n",
    "\n",
    "        for j in range(3,6):\n",
    "            df_activity[:,j] = signal.lfilter(b, a, df_activity[:, j])\n",
    "        #plot the gyroscope data\n",
    "        print(\"Gyroscope for activity\" + str(i))\n",
    "        plt.plot(df_activity[500:1500, 3:6])\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_removing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering - task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out features for all 19 sets of data \n",
    "def create_features():\n",
    "    b, a = signal.butter(4, 0.04, 'low', analog=False) # create a butterworth low pass filter\n",
    "\n",
    "    for i in range(19): #read all 19 datasets\n",
    "            df = pd.read_csv('dataset_' + str(i + 1) + '.txt', sep=',', header=None)\n",
    "            print(\"\") # used only to place a space between datasets \n",
    "            print('deal with dataset ' + str(i + 1))\n",
    "\n",
    "            for c in range(1, 14): #seperate each activity in each dataset\n",
    "                print(\"Activity\" + str(c))\n",
    "                Activity_data = df[df[24] == c].values\n",
    "\n",
    "                for j in range (24): # clean data of each sensor \n",
    "                    Activity_data[:,i] = signal.lfilter(b, a, Activity_data[:, i])\n",
    "\n",
    "                #create features\n",
    "                minVal = np.min(Activity_data[:, c])\n",
    "                maxVal = np.max(Activity_data[:, c])\n",
    "                meanVal = np.mean(Activity_data[:, c])\n",
    "                medianVal = np.median(Activity_data[:, c])\n",
    "\n",
    "                print(\"minVal = \" + str(minVal) + \", maxVal = \" + str(maxVal) +  \", meanVal = \" + str(meanVal)\n",
    "                      +  \", medianVal = \" + str(medianVal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering():\n",
    "    training = np.empty(shape=(0, 73))\n",
    "    testing = np.empty(shape=(0, 73))\n",
    "    \n",
    "    # declare the butterworth filter \n",
    "    b, a = signal.butter(4, 0.04, 'low', analog=False)\n",
    "    \n",
    "    # Read dataset from all 19 participants \n",
    "    for i in range(19):\n",
    "        df = pd.read_csv('dataset_' + str(i + 1) + '.txt', sep=',', header=None)\n",
    "        print('deal with dataset ' + str(i + 1))\n",
    "        \n",
    "        # Seperate each activity in each dataset\n",
    "        for c in range(1, 14): \n",
    "                print(\"Deal with activity\" + str(c))\n",
    "                activity_data = df[df[24] == c].values\n",
    "                \n",
    "                # Apply the butterworth filter to each sensor\n",
    "                for j in range(24):\n",
    "                    activity_data[:, j] = signal.lfilter(b, a, activity_data[:, j])\n",
    "\n",
    "                    # Seperate the training and testing sets \n",
    "                datat_len = len(activity_data)\n",
    "                training_len = math.floor(datat_len * 0.8) # seperates 80% of data for training and 20% for testing\n",
    "                training_data = activity_data[:training_len, :]\n",
    "                testing_data = activity_data[training_len:, :]\n",
    "\n",
    "                # Data segmentation - create feature sets \n",
    "                training_sample_number = training_len // 1000 + 1\n",
    "                testing_sample_number = (datat_len - training_len) // 1000 + 1\n",
    "                \n",
    "                for s in range(training_sample_number):\n",
    "                    if s < training_sample_number - 1:\n",
    "                        sample_data = training_data[1000*s:1000*(s + 1), :]\n",
    "                    else:\n",
    "                        sample_data = training_data[1000*s:, :]\n",
    "                \n",
    "                # For each sensor extract a feature of min, max and mean value then store it in list feature sample\n",
    "                # Feature sample is then added to training \n",
    "                    feature_sample = []\n",
    "                    for i in range(24):\n",
    "                        feature_sample.append(np.min(sample_data[:, i]))\n",
    "                        feature_sample.append(np.max(sample_data[:, i]))\n",
    "                        feature_sample.append(np.mean(sample_data[:, i]))\n",
    "                    feature_sample.append(sample_data[0, -1])\n",
    "                    feature_sample = np.array([feature_sample])\n",
    "                    training = np.concatenate((training, feature_sample), axis=0)\n",
    "\n",
    "                for s in range(testing_sample_number):\n",
    "                    if s < training_sample_number - 1:\n",
    "                        sample_data = testing_data[1000*s:1000*(s + 1), :]\n",
    "                    else:\n",
    "                        sample_data = testing_data[1000*s:, :]\n",
    "                    \n",
    "                    # For each sensor extract a feature of min, max and mean value then store it in list feature sample\n",
    "                    # Feature sample is then added to testing \n",
    "                    feature_sample = []\n",
    "                    for i in range(24):\n",
    "                        feature_sample.append(np.min(sample_data[:, i]))\n",
    "                        feature_sample.append(np.max(sample_data[:, i]))\n",
    "                        feature_sample.append(np.mean(sample_data[:, i]))\n",
    "                    feature_sample.append(sample_data[0, -1])\n",
    "                    feature_sample = np.array([feature_sample])\n",
    "                    testing = np.concatenate((testing, feature_sample), axis=0)\n",
    "\n",
    "    df_training = pd.DataFrame(training)\n",
    "    df_testing = pd.DataFrame(testing)\n",
    "    df_training.to_csv('training_data.csv', index=None, header=None)\n",
    "    df_testing.to_csv('testing_data.csv', index=None, header=None)\n",
    "                \n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineering()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
